<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Ling Yang ‚Äî Princeton University</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Instrument+Serif:ital@0;1&family=DM+Sans:ital,opsz,wght@0,9..40,300..700;1,9..40,300..700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
<style>
:root {
  --bg: #faf9f7; --bg-card: #ffffff; --text: #1a1a1a; --text-secondary: #555;
  --text-muted: #888; --accent: #2563eb; --accent-warm: #c2410c;
  --border: #e8e5e0; --border-light: #f0ede8; --tag-bg: #f0ede8;
  --shadow-sm: 0 1px 3px rgba(0,0,0,0.04); --shadow-md: 0 4px 20px rgba(0,0,0,0.06);
  --radius: 12px; --radius-sm: 8px;
  --serif: 'Instrument Serif', Georgia, serif;
  --sans: 'DM Sans', -apple-system, sans-serif;
  --mono: 'JetBrains Mono', monospace;
  --max-w: 1080px;
}
* { margin:0; padding:0; box-sizing:border-box; }
html { scroll-behavior:smooth; }
body { font-family:var(--sans); background:var(--bg); color:var(--text); line-height:1.7; font-size:17px; -webkit-font-smoothing:antialiased; }
::selection { background:#2563eb22; }
a { color:var(--accent); text-decoration:none; transition:color .2s; }
a:hover { color:var(--accent-warm); }

nav { position:sticky; top:0; z-index:100; background:rgba(250,249,247,.85); backdrop-filter:blur(16px); -webkit-backdrop-filter:blur(16px); border-bottom:1px solid var(--border-light); }
nav .inner { max-width:var(--max-w); margin:0 auto; display:flex; align-items:center; justify-content:space-between; padding:14px 32px; }
nav .logo { font-family:var(--serif); font-size:20px; color:var(--text); letter-spacing:-.02em; }
nav .links { display:flex; gap:24px; flex-wrap:wrap; }
nav .links a { font-size:14.5px; font-weight:500; color:var(--text-secondary); letter-spacing:.01em; position:relative; transition:color .2s; }
nav .links a:hover, nav .links a.active { color:var(--text); }
nav .links a.active::after { content:''; position:absolute; bottom:-4px; left:0; right:0; height:1.5px; background:var(--text); border-radius:1px; }
nav .mobile-toggle { display:none; background:none; border:none; font-size:22px; cursor:pointer; }
nav .princeton-badge { display:flex; align-items:center; gap:8px; text-decoration:none; border-bottom:none !important; }
nav .princeton-badge img { height:28px; width:auto; opacity:.9; }
nav .princeton-badge:hover img { opacity:1; }

.top-block { max-width:var(--max-w); margin:0 auto; padding:60px 32px 52px; display:flex; flex-direction:column; gap:40px; }
.hero { display:grid; grid-template-columns:240px 1fr; gap:44px; align-items:start; }
.hero-left { display:flex; flex-direction:column; align-items:center; gap:16px; animation:fadeUp .8s ease both; }
.hero-photo { width:240px; height:240px; border-radius:var(--radius); object-fit:cover; border:3px solid var(--border); box-shadow:var(--shadow-md); }
.hero-info { animation:fadeUp .8s ease .1s both; }
.hero-info h1 { font-family:var(--serif); font-size:46px; font-weight:400; letter-spacing:-.03em; line-height:1.15; margin-bottom:6px; }
.hero-info h1 .cn { font-size:30px; color:var(--text-muted); margin-left:8px; }
.hero-info .subtitle { font-size:17.5px; color:var(--text-secondary); margin-bottom:16px; }
.hero-info .subtitle strong { font-weight:600; color:var(--text); }
.hero-info .bio { font-size:17px; color:var(--text-secondary); line-height:1.75; margin-bottom:8px; text-align:justify; }
.hero-info .bio a { border-bottom:1px solid #2563eb33; }
.hero-info .bio a:hover { border-bottom-color:var(--accent); }
.social-links { display:flex; flex-direction:column; gap:6px; width:100%; }
.social-links a { display:inline-flex; align-items:center; justify-content:center; gap:6px; padding:7px 10px; border-radius:8px; font-size:13px; font-weight:500; background:var(--bg-card); color:var(--text-secondary); border:1px solid var(--border); transition:all .2s; width:100%; }
.social-links a:hover { border-color:var(--accent); color:var(--accent); box-shadow:var(--shadow-sm); transform:translateY(-1px); }
.social-links a svg { width:14px; height:14px; flex-shrink:0; }

section { max-width:var(--max-w); margin:0 auto; padding:0 32px 52px; }
.section-header { display:flex; align-items:baseline; gap:12px; margin-bottom:24px; }
.section-header h2 { font-family:var(--serif); font-size:31px; font-weight:400; letter-spacing:-.02em; white-space:nowrap; }
.section-header .line { flex:1; height:1px; background:var(--border); }
.section-header a { font-size:14.5px; white-space:nowrap; }

.recruit-inner { padding:28px 32px; border-radius:var(--radius); background:linear-gradient(135deg,#1e3a5f 0%,#1a1a2e 100%); color:#fff; position:relative; overflow:hidden; width:100%; box-sizing:border-box; }
.recruit-inner::before { content:''; position:absolute; top:-50%; right:-20%; width:300px; height:300px; background:radial-gradient(circle,rgba(99,102,241,.15) 0%,transparent 70%); border-radius:50%; }
.recruit-inner h3 { font-family:var(--serif); font-size:22px; font-weight:400; margin-bottom:8px; position:relative; }
.recruit-inner p { font-size:15px; opacity:.85; line-height:1.7; position:relative; }
.recruit-reqs { list-style:none; margin:14px 0 0; display:flex; flex-direction:column; gap:8px; position:relative; }
.recruit-reqs li { display:flex; align-items:baseline; gap:10px; font-size:14.5px; opacity:.9; line-height:1.55; }
.recruit-reqs li .req-num { font-family:var(--mono); font-size:11px; font-weight:700; background:rgba(255,255,255,.15); border-radius:4px; padding:1px 6px; flex-shrink:0; letter-spacing:.04em; }
.recruit-reqs li strong { color:#fff; font-weight:600; }
.recruit-inner .recruit-links { margin-top:20px; display:flex; gap:8px; position:relative; flex-wrap:wrap; }
.recruit-inner .recruit-links a { padding:9px 22px; border-radius:20px; font-size:14px; font-weight:600; background:#fff; color:#1a1a2e; transition:all .2s; }
.recruit-inner .recruit-links a:hover { transform:translateY(-1px); box-shadow:0 4px 12px rgba(0,0,0,.2); }
.recruit-inner .recruit-links a.outline { background:transparent; color:#fff; border:1px solid rgba(255,255,255,.3); }
.recruit-inner .recruit-links a.outline:hover { border-color:#fff; background:rgba(255,255,255,.08); }
.recruit-roles { display:flex; flex-wrap:wrap; gap:8px; margin:14px 0 0; position:relative; }
.recruit-role { padding:5px 13px; border-radius:20px; font-size:13px; font-weight:600; border:1px solid rgba(255,255,255,.25); color:#fff; display:flex; align-items:center; gap:6px; }
.recruit-role .role-label { opacity:.65; font-size:11px; font-weight:400; }
.recruit-unis { display:flex; flex-wrap:wrap; gap:6px; margin:10px 0 0; position:relative; }
.recruit-uni { padding:4px 12px; border-radius:6px; font-size:12.5px; font-weight:600; background:rgba(255,255,255,.1); color:rgba(255,255,255,.9); }

.research-goal { font-size:16.5px; color:var(--text-secondary); margin-bottom:28px; line-height:1.8; text-align:justify; }
.research-goal strong { color:var(--text); }
.pillars { display:grid; grid-template-columns:repeat(2,1fr); gap:14px; margin-bottom:20px; }
.pillar { padding:22px; background:var(--bg-card); border:1px solid var(--border); border-radius:var(--radius); transition:all .3s; }
.pillar:hover { border-color:var(--accent); box-shadow:var(--shadow-md); transform:translateY(-2px); }
.pillar h3 { font-size:15px; font-weight:600; margin-bottom:8px; display:flex; align-items:center; gap:8px; }
.pillar h3 .dot { width:8px; height:8px; border-radius:50%; flex-shrink:0; }
.pillar .paper-links { font-size:14px; color:var(--text-secondary); line-height:1.7; }
.pillar .paper-links a { font-weight:500; }
.sub-section-title { font-size:16.5px; font-weight:600; margin:28px 0 14px; color:var(--text); display:flex; align-items:center; gap:8px; }
.sub-section-title::before { content:''; width:3px; height:16px; border-radius:2px; background:var(--accent); }
.book-card { padding:22px; background:var(--bg-card); border:1px solid var(--border); border-radius:var(--radius); margin-top:20px; }
.book-card h3 { font-size:16px; font-weight:600; margin-bottom:4px; }
.book-card p { font-size:14.5px; color:var(--text-secondary); line-height:1.65; }

.news-list { display:flex; flex-direction:column; }
.news-list { display:flex; flex-direction:column; gap:2px; }
.news-item { display:flex; align-items:baseline; gap:14px; padding:9px 12px; border-radius:var(--radius-sm); transition:background .15s; }
.news-item:hover { background:var(--border-light); }
.news-item::before { content:'‚Äì'; color:var(--text-muted); font-size:14px; flex-shrink:0; }
.news-item .content { font-size:16px; color:var(--text-secondary); line-height:1.6; }
.news-item .content strong { color:var(--text); font-weight:600; }
.venue { display:inline-block; padding:2px 8px; border-radius:4px; font-size:12.5px; font-weight:600; letter-spacing:.02em; }
.venue-conf { background:#e0e7ff; color:#3730a3; }
.venue-oral { background:#fef3c7; color:#92400e; }
.venue-spotlight { background:#dbeafe; color:#1e40af; }
.venue-best { background:#fce7f3; color:#9d174d; }
.hidden-news { display:none; }
.hidden-news.show { display:flex; }
.show-more-btn { display:inline-flex; align-items:center; gap:4px; margin-top:12px; font-size:14.5px; font-weight:500; color:var(--accent); cursor:pointer; border:none; background:none; font-family:var(--sans); transition:color .2s; }
.show-more-btn:hover { color:var(--accent-warm); }

.pub-section-title { font-size:17.5px; font-weight:600; margin:28px 0 16px; padding-bottom:8px; border-bottom:1px solid var(--border); color:var(--text); }
.pub-section-title:first-of-type { margin-top:0; }
.pub-note { font-size:14.5px; color:var(--text-muted); margin-bottom:20px; font-style:italic; }
.pub-list { display:flex; flex-direction:column; gap:14px; }
.pub-item { padding:18px 20px; background:var(--bg-card); border:1px solid var(--border); border-radius:var(--radius); transition:all .3s; }
.pub-item:hover { box-shadow:var(--shadow-md); border-color:#d0d0d0; transform:translateY(-1px); }
.pub-meta { display:flex; align-items:center; gap:6px; margin-bottom:5px; flex-wrap:wrap; }
.pub-venue-tag { font-family:var(--mono); font-size:12px; font-weight:600; padding:2px 8px; border-radius:4px; letter-spacing:.03em; }
.pub-venue-tag.neurips { background:#ede9fe; color:#5b21b6; }
.pub-venue-tag.icml { background:#dbeafe; color:#1e40af; }
.pub-venue-tag.iclr { background:#d1fae5; color:#065f46; }
.pub-venue-tag.iccv { background:#fce7f3; color:#9d174d; }
.pub-venue-tag.cvpr { background:#fef3c7; color:#92400e; }
.pub-venue-tag.acm { background:#f3e8ff; color:#6b21a8; }
.pub-venue-tag.emnlp { background:#fef9c3; color:#854d0e; }
.pub-venue-tag.preprint { background:#f5f5f4; color:#78716c; }
.pub-award-tag { font-size:11.5px; font-weight:700; padding:2px 8px; border-radius:4px; background:#fef2f2; color:#b91c1c; }
.pub-item h3 { font-size:16px; font-weight:600; line-height:1.45; letter-spacing:-.01em; margin-bottom:4px; }
.pub-authors { font-size:14px; color:var(--text-secondary); line-height:1.5; margin-bottom:8px; }
.pub-authors .me { font-weight:700; color:var(--text); }
.pub-links { display:flex; flex-wrap:wrap; gap:5px; }
.pub-links a { font-family:var(--mono); font-size:12.5px; padding:3px 10px; border-radius:5px; background:var(--tag-bg); color:var(--text-secondary); border:1px solid transparent; transition:all .2s; }
.pub-links a:hover { background:var(--bg); border-color:var(--border); color:var(--accent); }

.honors-grid { display:grid; grid-template-columns:repeat(2,1fr); gap:10px; }
.honor-item { padding:16px 18px; background:var(--bg-card); border:1px solid var(--border); border-radius:var(--radius-sm); display:flex; align-items:flex-start; gap:12px; transition:all .2s; }
.honor-item:hover { box-shadow:var(--shadow-sm); }
.honor-icon { width:28px; height:28px; flex-shrink:0; border-radius:6px; background:#fef3c7; display:flex; align-items:center; justify-content:center; font-size:14px; }
.honor-text h4 { font-size:14.5px; font-weight:600; line-height:1.4; }
.honor-text p { font-size:13px; color:var(--text-muted); margin-top:1px; }

.talks-list { display:flex; flex-direction:column; }
.talk-item { padding:12px 0; border-bottom:1px solid var(--border-light); font-size:16px; color:var(--text-secondary); line-height:1.65; display:flex; align-items:baseline; gap:12px; }
.talk-item:last-child { border-bottom:none; }
.talk-year { font-family:var(--mono); font-size:14px; color:var(--text-muted); flex-shrink:0; }

.services-grid { display:grid; grid-template-columns:repeat(3,1fr); gap:20px; }
.service-group h3 { font-size:15px; font-weight:600; margin-bottom:8px; display:flex; align-items:center; gap:8px; }
.service-group h3::before { content:''; width:3px; height:14px; border-radius:2px; background:var(--accent); }
.service-group p, .service-group ul { font-size:14.5px; color:var(--text-secondary); line-height:1.7; }
.service-group ul { list-style:none; }
.service-group ul li::before { content:'¬∑ '; color:var(--text-muted); }

footer { max-width:var(--max-w); margin:0 auto; padding:28px 32px; border-top:1px solid var(--border); display:flex; justify-content:space-between; align-items:center; flex-wrap:wrap; gap:12px; }
footer p { font-size:13.5px; color:var(--text-muted); }
footer .foot-links { display:flex; gap:16px; flex-wrap:wrap; }
footer .foot-links a { font-size:13.5px; color:var(--text-muted); }
footer .foot-links a:hover { color:var(--text); }

@media (max-width:768px) {
  .top-block { padding:36px 20px 36px; gap:28px; }
  .hero { grid-template-columns:1fr; gap:20px; text-align:center; }
  .hero-left { align-items:center; }
  .hero-photo { margin:0 auto; width:160px; height:160px; border-radius:var(--radius-sm); }
  .social-links { flex-direction:row; flex-wrap:wrap; justify-content:center; }
  .social-links a { width:auto; }
  .hero-info h1 { font-size:34px; }
  section, .recruit { padding-left:20px; padding-right:20px; }
  .pillars { grid-template-columns:1fr; }
  .honors-grid { grid-template-columns:1fr; }
  .services-grid { grid-template-columns:1fr; }
  nav .links { display:none; }
  nav .mobile-toggle { display:block; }
  nav.open .links { display:flex; flex-direction:column; position:absolute; top:100%; left:0; right:0; background:var(--bg); padding:16px 32px; border-bottom:1px solid var(--border); gap:12px; }
}
@keyframes fadeUp { from { opacity:0; transform:translateY(16px); } to { opacity:1; transform:translateY(0); } }
</style>
</head>
<body>

<nav id="navbar">
  <div class="inner">
    <a href="#" class="logo">Ling Yang</a>
    <button class="mobile-toggle" onclick="document.getElementById('navbar').classList.toggle('open')">&#9776;</button>
    <div class="links">
      <a href="#" class="active">About</a>
      <a href="#research">Research</a>
      <a href="#news">News</a>
      <a href="#publications">Publications</a>
      <a href="#talks">Talks</a>
      <a href="#honors">Honors</a>
      <a href="#services">Services</a>
    </div>
    <a href="https://www.princeton.edu" class="princeton-badge" target="_blank">
      <img src="https://yangling0818.github.io/images/prince.png" alt="Princeton University">
    </a>
  </div>
</nav>

<div class="top-block">

<header class="hero">
  <div class="hero-left">
    <img src="https://yangling0818.github.io/images/ling_2.jpg" alt="Ling Yang" class="hero-photo">
    <div class="social-links">
      <a href="mailto:yangling0818@163.com"><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="2" y="4" width="20" height="16" rx="2"/><polyline points="22,7 12,13 2,7"/></svg>Email</a>
      <a href="https://yangling0818.github.io/_pages/image-1.png">üí¨ WeChat</a>
      <a href="https://github.com/YangLing0818"><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z"/></svg>GitHub</a>
      <a href="https://github.com/Gen-Verse"><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z"/></svg>Gen-Verse</a>
      <a href="https://scholar.google.com.hk/citations?user=sIKujqAAAAAJ&hl=en"><svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 24a7 7 0 110-14 7 7 0 010 14zm0-24L0 9.5l4.838 3.94A8 8 0 0112 9a8 8 0 017.162 4.44L24 9.5z"/></svg>Scholar</a>
      <a href="https://x.com/LingYang_PU"><svg viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>Twitter/X</a>
      <a href="https://huggingface.co/Gen-Verse">ü§ó HuggingFace</a>
      <a href="https://www.xiaohongshu.com/user/profile/5bf9033627150f0001533e35">Â∞èÁ∫¢‰π¶</a>
    </div>
  </div>
  <div class="hero-info">
    <h1>Ling Yang <span class="cn">Êù®ÁÅµ</span></h1>
    <p class="subtitle">Postdoctoral Scholar &middot; <strong>Princeton University</strong></p>
    <p class="bio">I am currently a Postdoctoral Scholar in the <a href="https://ece.princeton.edu/">Department of ECE at Princeton University</a>, co-affiliated with <a href="https://ai.princeton.edu/ai-lab">Princeton AI Lab</a>, fortunately working with Prof. <a href="https://mwang.princeton.edu/">Mengdi Wang</a>. Prior to this, I served as a Senior Research Assistant at Princeton University from January to July 2025. I received my Ph.D. degree from Peking University in July 2025, jointly supervised by Prof. <a href="https://cuibinpku.github.io/">Bin Cui</a> and Prof. <a href="https://www.nihds.pku.edu.cn/en/info/1027/1002.htm">Luxia Zhang</a>. During my doctoral studies, I was selected for the <a href="https://seed.bytedance.com/en/topseed?view_from=homepage_tab">ByteDance Top Seed Talent Program</a>.</p>
    <p class="bio">I was also fortunate to collaborate with <a href="https://yang-song.net/">Yang Song</a>, <a href="https://scholar.google.com.hk/citations?user=DNuiPHwAAAAJ&hl=zh-CN">Shuicheng Yan</a>, <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=zh-CN">Ming-Hsuan Yang</a>, <a href="https://scholar.google.com/citations?user=rVsGTeEAAAAJ&hl=zh-CN">Bernard Ghanem</a>, <a href="https://scholar.google.com/citations?user=2efgcS0AAAAJ&hl=zh-CN">Jiajun Wu</a>, <a href="https://scholar.google.com/citations?user=ogXTOZ4AAAAJ&hl=en">Stefano Ermon</a>, <a href="https://scholar.google.com/citations?user=Q_kKkIUAAAAJ&hl=zh-CN">Jure Leskovec</a>, <a href="https://scholar.google.com/citations?user=vhP-tlcAAAAJ&hl=zh-CN">Yejin Choi</a>, and <a href="https://scholar.google.com.hk/citations?user=23ZXZvEAAAAJ&hl=zh-CN">James Zou</a>.</p>
    <p class="bio">I currently focus on developing advanced generative models, including their training methodologies, architecture design, alignment, inference efficiency and applications. I am in charge of <a href="https://github.com/Gen-Verse">a research team</a> at Princeton and have led a series of works on LLMs/MLLMs and Diffusion Models, including <a href="https://openreview.net/forum?id=DgLFkAPwuZ">RPG-DiffusionMaster</a> <img src="https://img.shields.io/github/stars/YangLing0818/RPG-DiffusionMaster" alt="stars" style="vertical-align:middle;height:16px;">, <a href="https://arxiv.org/abs/2505.15809">MMaDA</a> <img src="https://img.shields.io/github/stars/Gen-Verse/MMaDA" alt="stars" style="vertical-align:middle;height:16px;">, <a href="https://arxiv.org/pdf/2406.04271">Buffer of Thoughts</a> <img src="https://img.shields.io/github/stars/YangLing0818/buffer-of-thought-llm" alt="stars" style="vertical-align:middle;height:16px;">, <a href="https://github.com/Gen-Verse/ReasonFlux">ReasonFlux/PRM/Coder</a> <img src="https://img.shields.io/github/stars/Gen-Verse/ReasonFlux" alt="stars" style="vertical-align:middle;height:16px;">, <a href="https://arxiv.org/abs/2511.20639">LatentMAS</a> <img src="https://img.shields.io/github/stars/Gen-Verse/LatentMAS" alt="stars" style="vertical-align:middle;height:16px;">, <a href="https://arxiv.org/abs/2509.06949">dLLM-RL</a> <img src="https://img.shields.io/github/stars/Gen-Verse/dLLM-RL" alt="stars" style="vertical-align:middle;height:16px;">, <a href="https://arxiv.org/abs/2407.02398">Consistency Flow Matching</a> <img src="https://img.shields.io/github/stars/YangLing0818/consistency_flow_matching" alt="stars" style="vertical-align:middle;height:16px;">, <a href="https://arxiv.org/abs/2602.02488">RLAnything</a> <img src="https://img.shields.io/github/stars/Gen-Verse/Open-AgentRL" alt="stars" style="vertical-align:middle;height:16px;">.</p>
  </div>
</header>

<div class="recruit-inner">
  <h3>Open Positions</h3>
  <p>We are looking for collaborators for research in <strong>LLM/MLLM Post-Training, Diffusion LLMs, World Modeling, and Agent Training</strong>. Positions are available across multiple levels and institutions:</p>
  <div class="recruit-roles">
    <div class="recruit-role">Intern <span class="role-label">Research</span></div>
    <div class="recruit-role">Master's <span class="role-label">Student</span></div>
    <div class="recruit-role">PhD <span class="role-label">Student</span></div>
    <div class="recruit-role">Postdoc <span class="role-label">Researcher</span></div>
  </div>
  <div class="recruit-unis">
    <span class="recruit-uni">üéì Peking University</span>
    <span class="recruit-uni">üéì Princeton University</span>
    <span class="recruit-uni">üéì Stanford University</span>
  </div>
  <p style="margin-top:18px; position:relative;">We take collaboration seriously ‚Äî please make sure the following apply before reaching out:</p>
  <ul class="recruit-reqs">
    <li><span class="req-num">01</span><span><strong>Self-motivated</strong> ‚Äî you drive your own execution and don't need to be pushed to make progress.</span></li>
    <li><span class="req-num">02</span><span><strong>Full commitment of 3+ months</strong> ‚Äî you can dedicate yourself entirely to the project without major competing obligations.</span></li>
    <li><span class="req-num">03</span><span><strong>Fast execution</strong> ‚Äî we define the research direction; you move quickly, iterate efficiently, and deliver results.</span></li>
  </ul>
  <p style="margin-top:16px; position:relative;">In return, we offer:</p>
  <ul class="recruit-reqs">
    <li><span class="req-num">‚ú¶</span><span><strong>Frontier research perspective</strong> ‚Äî direct exposure to the latest ideas and ongoing work at the cutting edge of generative AI.</span></li>
    <li><span class="req-num">‚ú¶</span><span><strong>Substantial compute resources</strong> ‚Äî access to the compute you need to run large-scale experiments without bottlenecks.</span></li>
    <li><span class="req-num">‚ú¶</span><span><strong>Strong connections</strong> ‚Äî academic network spanning Stanford, Princeton, MIT, PKU, Tsinghua and other top institutions, plus industry partners including Google DeepMind, ByteDance, Apple, Microsoft, and Meta.</span></li>
  </ul>
  <div class="recruit-links">
    <a href="mailto:yangling0818@163.com">Contact via Email</a>
    <a href="https://yangling0818.github.io/_pages/image-1.png" class="outline">WeChat QR</a>
    <a href="https://github.com/Gen-Verse" class="outline">Gen-Verse Team</a>
  </div>
</div>

</div>

<section id="research">
  <div class="section-header"><h2>Research Summary</h2><div class="line"></div></div>
  <p class="research-goal">My overarching goal is to build <strong>a unified system for Physics-Aware Generative Intelligence</strong> ‚Äî advancing AI from passive pattern recognition toward active reasoning, simulation, and discovery in the physical world.</p>
  <p class="research-goal" style="margin-top:10px;">This vision is organized around <strong>two complementary pillars</strong>. The first, <strong>Generative Model Foundations</strong>, pursues the core algorithmic and architectural advances that power the next generation of AI ‚Äî spanning language model reasoning, large-scale reinforcement learning, intelligent agent systems, and diffusion model innovations. The second, <strong>Generative Applications</strong>, deploys these foundations to tackle real-world challenges: generating coherent multimodal content across image, 3D, and 4D domains, and accelerating scientific discovery through AI-driven hypothesis generation and evaluation.</p>
  <p class="research-goal" style="margin-top:10px;">The two pillars are united by a shared philosophy: <em>theoretical depth should translate directly into empirical impact</em>, and generalization across modalities and scenarios is the ultimate test of any AI system's intelligence.</p>

  <div class="sub-section-title">Generative Model Foundations</div>
  <div class="pillars">
    <div class="pillar"><h3><span class="dot" style="background:#ec4899"></span>Language Model Innovations</h3><p class="paper-links"><a href="https://arxiv.org/pdf/2406.04271">Buffer of Thought</a>, <a href="https://github.com/Gen-Verse/MMaDA">MMaDA</a>, <a href="https://github.com/Gen-Verse/ReasonFlux">ReasonFlux</a>, <a href="https://github.com/Gen-Verse/CURE">ReasonFlux-Coder</a>, <a href="https://github.com/Gen-Verse/ReasonFlux">ReasonFlux-PRM</a>, <a href="https://arxiv.org/abs/2509.06949">TraDo</a>, <a href="https://arxiv.org/abs/2410.09008">SuperCorrect</a>, <a href="https://arxiv.org/abs/2511.20639">LatentMAS</a>, <a href="https://arxiv.org/abs/2512.13278">AutoTool</a>, <a href="https://arxiv.org/abs/2602.02488">RLAnything</a></p></div>
    <div class="pillar"><h3><span class="dot" style="background:#10b981"></span>Large-Scale Reinforcement Learning</h3><p class="paper-links"><a href="https://github.com/Gen-Verse/dLLM-RL">dLLM-RL</a>, <a href="https://github.com/Gen-Verse/MMaDA">MMaDA</a>, <a href="https://github.com/Gen-Verse/CURE">ReasonFlux-Coder</a>, <a href="https://github.com/Gen-Verse/ReasonFlux">ReasonFlux-PRM</a>, <a href="https://arxiv.org/abs/2502.12148">HermesFlow</a>, <a href="https://arxiv.org/abs/2510.11701">Demystifying Agent RL</a>, <a href="https://arxiv.org/abs/2602.02488">RLAnything</a></p></div>
    <div class="pillar"><h3><span class="dot" style="background:#f59e0b"></span>Intelligent Agent Systems</h3><p class="paper-links"><a href="https://arxiv.org/abs/2505.20286">Alita</a>, <a href="https://arxiv.org/abs/2506.14728">AgentDistill</a>, <a href="https://github.com/Gen-Verse/ScoreFlow">ScoreFlow</a>, <a href="https://arxiv.org/abs/2410.08102">Multi-Actor Collaboration</a>, <a href="https://arxiv.org/abs/2508.09632">Preacher</a>, <a href="https://arxiv.org/abs/2504.09689">EmoAgent</a>, <a href="https://arxiv.org/abs/2510.11701">DemyAgent</a>, <a href="https://arxiv.org/abs/2511.20639">LatentMAS</a>, <a href="https://github.com/Gen-Verse/GenEnv">GenEnv</a>, <a href="https://arxiv.org/abs/2602.02488">RLAnything</a></p></div>
    <div class="pillar"><h3><span class="dot" style="background:#6366f1"></span>Diffusion Model Foundations</h3><p class="paper-links"><a href="https://openreview.net/forum?id=DgLFkAPwuZ">RPG</a>, <a href="https://github.com/Gen-Verse/MMaDA">MMaDA</a>, <a href="https://openreview.net/forum?id=nFMS6wF2xq">ContextDiff</a>, <a href="https://arxiv.org/abs/2509.06949">dLLM-RL</a>, <a href="https://arxiv.org/abs/2407.02398">Consistency Flow Matching</a>, <a href="https://arxiv.org/abs/2502.12146">Diffusion-Sharpening</a>, <a href="https://openreview.net/forum?id=nEDToD1R8M">Rectified Diffusion</a>, <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/7664a7e946a84ac5e97649a967717cf2-Abstract-Conference.html">ConPreDiff</a>, <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Structure-Guided_Adversarial_Training_of_Diffusion_Models_CVPR_2024_paper.html">SADM</a>, <a href="https://github.com/tyfeld/MMaDA-Parallel">MMaDA-Parallel</a></p></div>
  </div>

  <div class="sub-section-title">Generative Applications</div>
  <div class="pillars">
    <div class="pillar"><h3><span class="dot" style="background:#8b5cf6"></span>Multimodal Content Generation</h3><p class="paper-links"><a href="https://openreview.net/forum?id=4w99NAikOE">IterComp</a>, <a href="https://arxiv.org/abs/2406.04277">VideoTetris</a>, <a href="https://github.com/happyw1nd/ScoreLiDAR">ScoreLiDAR</a>, <a href="https://openreview.net/forum?id=3PguviI7Uf">IPDreamer</a>, <a href="https://arxiv.org/abs/2405.14785">EditWorld</a>, <a href="https://arxiv.org/abs/2410.07155">Trans4D</a>, <a href="https://arxiv.org/abs/2503.13435">WideRange4D</a>, <a href="https://arxiv.org/abs/2510.13804">OmniVerifier</a>, <a href="https://github.com/tyfeld/MMaDA-Parallel">MMaDA-Parallel</a>, <a href="https://github.com/thu-ml/DiT-Extrapolation">UltraViCo</a></p></div>
    <div class="pillar"><h3><span class="dot" style="background:#06b6d4"></span>AI for Scientific Discovery</h3><p class="paper-links"><a href="https://openreview.net/forum?id=qH9nrMNTIW">IPDiff</a>, <a href="https://openreview.net/forum?id=eejhD9FCP3">IRDiff</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29162">BindDM</a>, <a href="https://arxiv.org/abs/2505.19501">RL for Scientific Reasoning</a></p></div>
  </div>

  <div class="book-card">
    <h3>üìñ Book Publication</h3>
    <p><strong>"Diffusion Model: Theory, Application, and Code Practice of Generative AI Models"</strong><br>Published by Electronics Industry Press (ÁîµÂ≠êÂ∑•‰∏öÂá∫ÁâàÁ§æ), 2023 &middot; <a href="https://item.m.jd.com/product/14075554.html">Purchase Link</a> &middot; Selected as <strong>Annual Outstanding Author</strong></p>
  </div>
</section>

<section id="news">
  <div class="section-header"><h2>What's New</h2><div class="line"></div></div>
  <div class="news-list">
    <div class="news-item"><div class="content"><strong>2 papers</strong> about multimodal reinforcement learning are accepted by <span class="venue venue-conf">CVPR 2026</span>.</div></div>
    <div class="news-item"><div class="content"><strong>4 papers</strong> about LLMs and Diffusion Language Models are accepted by <span class="venue venue-conf">ICLR 2026</span>, including <a href="https://arxiv.org/abs/2509.06949">TraceRL &amp; TraDo</a>, <a href="https://arxiv.org/abs/2510.13804">OmniVerifier</a> <span class="venue venue-oral">Oral, Top 1%</span>, <a href="https://arxiv.org/abs/2511.09611">MMaDA-Parallel</a> and <a href="https://github.com/thu-ml/DiT-Extrapolation">UltraViCo</a>.</div></div>
    <div class="news-item"><div class="content">We release a series of works about efficient agent reasoning and agentic training: <a href="https://arxiv.org/abs/2511.20639">LatentMAS</a>, <a href="https://arxiv.org/abs/2602.02488">RLAnything</a>, and <a href="https://github.com/Gen-Verse/GenEnv">GenEnv</a>.</div></div>
    <div class="news-item"><div class="content">Our <a href="https://arxiv.org/abs/2512.13278">AutoTool</a> won <span class="venue venue-best">Best Paper Award</span> at <a href="https://agent-intelligence.github.io/agent-intelligence/">ICCV 2025 Workshop on MMRAgI</a>.</div></div>
    <div class="news-item"><div class="content">Invited to give a talk at <a href="https://agent-intelligence.github.io/agent-intelligence/">ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic Intelligence</a>.</div></div>
    <div class="news-item"><div class="content"><strong>5 papers</strong> about LLMs and Multimodal LLMs are accepted by <span class="venue venue-conf">NeurIPS 2025</span>, including <a href="https://github.com/Gen-Verse/MMaDA">MMaDA</a>, <a href="https://github.com/Gen-Verse/ReasonFlux">CURE &amp; ReasonFlux-Coder</a> <span class="venue venue-spotlight">Spotlight, Top 3%</span>, <a href="https://github.com/Gen-Verse/ReasonFlux">ReasonFlux-PRM</a>, <a href="https://arxiv.org/abs/2505.16270">Transformer-Copilot</a> <span class="venue venue-spotlight">Spotlight, Top 3%</span> and <a href="https://arxiv.org/abs/2502.12148">HermesFlow</a>.</div></div>
    <div class="news-item"><div class="content"><strong>3 papers</strong> about LLMs and Agents are accepted by <span class="venue venue-conf">EMNLP 2025</span>, including <a href="https://arxiv.org/abs/2504.09689">EmoAgent</a> <span class="venue venue-oral">Oral, Top 1%</span> and <a href="https://arxiv.org/abs/2410.16033">TreeBoN</a>.</div></div>
    <div class="news-item"><div class="content"><strong>2 papers</strong> about diffusion are accepted by <span class="venue venue-conf">ACM MM 2025</span>, including <a href="https://arxiv.org/abs/2507.11554">Inversion-DPO</a> and <a href="https://arxiv.org/abs/2405.14785">EditWorld</a>.</div></div>
    <div class="news-item"><div class="content">I was selected as a finalist for the <a href="https://www.thegaiaa.org/en/awards_mrzx">2025 WAIC Yunfan Award</a>.</div></div>
    <div class="news-item"><div class="content">Invited to participate in a roundtable forum at WAIC 2025, hosted by Prof. <a href="https://scholar.google.com/citations?user=GMzzRRUAAAAJ&hl=en">Dahua Lin</a>.</div></div>
    <div class="news-item"><div class="content"><strong>2 papers</strong> about agent and diffusion are accepted by <span class="venue venue-conf">ICCV 2025</span>, including <a href="https://github.com/happyw1nd/ScoreLiDAR">ScoreLiDAR</a> <span class="venue venue-oral">Oral, Top 1%</span> and <a href="https://arxiv.org/abs/2508.09632">Paper2Video Agent</a>.</div></div>
    <div class="news-item"><div class="content"><strong>1 paper</strong> about LLMs is accepted by <span class="venue venue-conf">ACL 2025</span>, including <a href="https://arxiv.org/abs/2410.08102">Multi-Actor Collaboration</a>.</div></div>
    <div class="news-item"><div class="content">I was invited as an <strong>Area Chair</strong> at <strong>NeurIPS 2025</strong>.</div></div>
    <div class="news-item"><div class="content"><strong>6 papers</strong> about LLMs and Diffusion Models are accepted by <span class="venue venue-conf">ICLR 2025</span>, including <a href="https://arxiv.org/abs/2410.09008">SuperCorrect</a>, <a href="https://openreview.net/forum?id=nEDToD1R8M">Rectified Diffusion</a>, <a href="https://openreview.net/forum?id=4w99NAikOE">IterComp</a> and <a href="https://openreview.net/forum?id=3PguviI7Uf">IPDreamer</a>.</div></div>
    <div class="news-item"><div class="content">Invited to give a talk at Princeton AI Lab, hosted by Prof. <a href="https://ece.princeton.edu/people/mengdi-wang">Mengdi Wang</a>.</div></div>
    <div class="news-item"><div class="content"><strong>5 papers</strong> about Diffusion Models and LLMs are accepted by <span class="venue venue-conf">NeurIPS 2024</span>, including <a href="https://github.com/YangLing0818/buffer-of-thought-llm">Buffer of Thoughts</a> <span class="venue venue-spotlight">Spotlight, Top 3%</span>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>2 papers</strong> about Diffusion Models and AI for Science are accepted by <span class="venue venue-conf">ICML 2024</span>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>1 paper</strong> about general/molecular graph diffusion is accepted by <strong>TKDE 2024</strong>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>1 paper</strong> about improved training algorithm of DiT, DDPMs and Score SDEs is accepted by <span class="venue venue-conf">CVPR 2024</span>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>3 papers</strong> about Diffusion Models, GNN, AI for Science are accepted by <span class="venue venue-conf">ICLR 2024</span>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>1 paper</strong> about molecular diffusion models is accepted by <span class="venue venue-conf">AAAI 2024</span>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>1 paper</strong> about diffusion model survey collaborating with OpenAI is accepted by <strong>ACM Computing Surveys</strong>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>1 paper</strong> about diffusion models is accepted by <span class="venue venue-conf">NeurIPS 2023</span>.</div></div>
    <div class="news-item hidden-news"><div class="content">I publish <a href="https://item.m.jd.com/product/14075554.html">a book about Diffusion Models</a>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>1 paper</strong> is accepted by <strong>TNNLS 2023</strong>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>1 paper</strong> is accepted by <strong>TKDE 2023</strong>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>2 papers</strong> are accepted as <span class="venue venue-conf">ICML 2022</span> <span class="venue venue-spotlight">Spotlight, Top 3%</span>.</div></div>
    <div class="news-item hidden-news"><div class="content"><strong>1 paper</strong> is accepted by <span class="venue venue-conf">CVPR 2020</span>.</div></div>
  </div>
  <button class="show-more-btn" onclick="toggleNews()">Show more &darr;</button>
</section>

<section id="publications">
  <div class="section-header"><h2>Selected Publications</h2><div class="line"></div><a href="https://scholar.google.com.hk/citations?user=sIKujqAAAAAJ&hl=en">[Full list &rarr;]</a></div>
  <p class="pub-note">*Co-first author, +Corresponding author. For a complete list, see my <a href="https://scholar.google.com.hk/citations?user=sIKujqAAAAAJ&hl=en">Google Scholar profile</a>.</p>

  <div class="pub-section-title">Recent Highlighted Work</div>
  <div class="pub-list">
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag preprint">Preprint</span></div><h3>RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</h3><p class="pub-authors">Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, <span class="me">Ling Yang</span>+</p><div class="pub-links"><a href="https://www.arxiv.org/abs/2602.02488">Paper</a><a href="https://github.com/Gen-Verse/Open-AgentRL">Code</a><a href="https://x.com/YinjieW2024/status/2018549477884166558?s=20">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag preprint">Preprint</span></div><h3>Latent Collaboration in Multi-Agent Systems</h3><p class="pub-authors">Jiaru Zou, Xiyuan Yang, Ruizhong Qiu, Gaotang Li, Katherine Tieu, Pan Lu, Ke Shen, Hanghang Tong, Yejin Choi, Jingrui He, James Zou, Mengdi Wang, <span class="me">Ling Yang</span>+</p><div class="pub-links"><a href="https://arxiv.org/abs/2511.20639">Paper</a><a href="https://github.com/Gen-Verse/LatentMAS">Code</a><a href="https://x.com/stanfordnlp/status/1996354695913492889?s=20">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag neurips">NeurIPS 2024</span><span class="pub-award-tag">Spotlight Top 3%</span></div><h3>Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models</h3><p class="pub-authors"><span class="me">Ling Yang</span>+, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui</p><div class="pub-links"><a href="https://arxiv.org/pdf/2406.04271">Paper</a><a href="https://github.com/YangLing0818/buffer-of-thought-llm">Code</a><a href="https://x.com/omarsar0/status/1799113545696567416">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag icml">ICML 2024</span></div><h3>Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs</h3><p class="pub-authors"><span class="me">Ling Yang</span>+, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui</p><div class="pub-links"><a href="https://openreview.net/forum?id=DgLFkAPwuZ">Paper</a><a href="https://github.com/YangLing0818/RPG-DiffusionMaster">Code</a><a href="https://x.com/_akhaliq/status/1749633221514461489">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag neurips">NeurIPS 2025</span></div><h3>MMaDA: Multimodal Large Diffusion Language Models</h3><p class="pub-authors"><span class="me">Ling Yang</span>+, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, Mengdi Wang</p><div class="pub-links"><a href="https://arxiv.org/abs/2505.15809">Paper</a><a href="https://github.com/Gen-Verse/MMaDA">Code</a><a href="https://x.com/_akhaliq/status/1925384556279898139">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag iclr">ICLR 2026</span></div><h3>Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models</h3><p class="pub-authors">Yinjie Wang, <span class="me">Ling Yang</span>*+, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang</p><div class="pub-links"><a href="https://arxiv.org/abs/2509.06949">Paper</a><a href="https://github.com/Gen-Verse/dLLM-RL">Code</a><a href="https://x.com/_akhaliq/status/1965422743194927429">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag preprint">Preprint</span></div><h3>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</h3><p class="pub-authors"><span class="me">Ling Yang</span>+, Zhaochen Yu, Bin Cui, Mengdi Wang</p><div class="pub-links"><a href="https://arxiv.org/abs/2502.06772">Paper</a><a href="https://github.com/Gen-Verse/ReasonFlux">Code</a><a href="https://x.com/_akhaliq/status/1889187356651012599">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag neurips">NeurIPS 2025</span><span class="pub-award-tag">Spotlight Top 3%</span></div><h3>Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning (ReasonFlux-Coder)</h3><p class="pub-authors">Yinjie Wang, <span class="me">Ling Yang</span>*+, Ye Tian, Ke Shen, Mengdi Wang</p><div class="pub-links"><a href="https://arxiv.org/abs/2506.03136">Paper</a><a href="https://github.com/Gen-Verse/CURE">Code</a><a href="https://x.com/_akhaliq/status/1930281721926234437">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag neurips">NeurIPS 2025</span></div><h3>ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs</h3><p class="pub-authors">Jiaru Zou, <span class="me">Ling Yang</span>*+, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang</p><div class="pub-links"><a href="https://arxiv.org/abs/2506.18896">Paper</a><a href="https://github.com/Gen-Verse/ReasonFlux">Code</a><a href="https://x.com/_akhaliq/status/1937345023005048925">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag iclr">ICLR 2025</span></div><h3>SuperCorrect: Advancing Small LLM Reasoning with Thought Template Distillation and Self-Correction</h3><p class="pub-authors"><span class="me">Ling Yang</span>+, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E. Gonzalez, Bin Cui, Shuicheng Yan</p><div class="pub-links"><a href="https://openreview.net/forum?id=PyjZO7oSw2">Paper</a><a href="https://github.com/YangLing0818/SuperCorrect-llm">Code</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag neurips">NeurIPS 2025</span><span class="pub-award-tag">Spotlight Top 3%</span></div><h3>Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning</h3><p class="pub-authors">Jiaru Zou, Yikun Ban, Zihao Li, Yunzhe Qi, Ruizhong Qiu, <span class="me">Ling Yang</span>+, Jingrui He</p><div class="pub-links"><a href="https://arxiv.org/abs/2505.16270">Paper</a><a href="https://github.com/jiaruzouu/TransformerCopilot">Code</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag iclr">ICLR 2026</span><span class="pub-award-tag">Oral Top 1%</span></div><h3>Generative Universal Verifier as Multimodal Meta-Reasoner</h3><p class="pub-authors">Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, <span class="me">Ling Yang</span>, Yujiu Yang</p><div class="pub-links"><a href="https://arxiv.org/abs/2510.13804">Paper</a><a href="https://github.com/Cominclip/OmniVerifier">Code</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag iccv">ICCV 2025</span><span class="pub-award-tag">Oral Top 1%</span></div><h3>Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion</h3><p class="pub-authors">Shengyuan Zhang, An Zhao, <span class="me">Ling Yang</span>, Zejian Li, Chenye Meng, Haoran Xu, Tianrun Chen, AnYang Wei, Perry Pengyun Gu, Lingyun Sun</p><div class="pub-links"><a href="https://arxiv.org/abs/2412.03515">Paper</a><a href="https://github.com/happyw1nd/ScoreLiDAR">Code</a></div></div>
  </div>

  <div class="pub-section-title">Core Contributions to Generative Foundations and Applications</div>
  <div class="pub-list">
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag neurips">NeurIPS 2025</span></div><h3>HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation</h3><p class="pub-authors"><span class="me">Ling Yang</span>+, Xinchen Zhang, Ye Tian, Chenming Shang, Minghao Xu, Wentao Zhang, Bin Cui</p><div class="pub-links"><a href="https://arxiv.org/abs/2502.12148">Paper</a><a href="https://github.com/Gen-Verse/HermesFlow">Code</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag iclr">ICLR 2025</span></div><h3>IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation</h3><p class="pub-authors">Xinchen Zhang*, <span class="me">Ling Yang</span>*, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, Bin Cui</p><div class="pub-links"><a href="https://arxiv.org/abs/2410.07171">Paper</a><a href="https://github.com/YangLing0818/IterComp">Code</a><a href="https://x.com/_akhaliq/status/1844272544687509910">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag iclr">ICLR 2025</span></div><h3>Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow</h3><p class="pub-authors">Fu-Yun Wang, <span class="me">Ling Yang</span>, Zhaoyang Huang, Mengdi Wang, Hongsheng Li</p><div class="pub-links"><a href="https://openreview.net/forum?id=nEDToD1R8M">Paper</a><a href="https://github.com/G-U-N/Rectified-Diffusion">Code</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag preprint">Preprint</span></div><h3>Consistency Flow Matching: Defining Straight Flows with Velocity Consistency</h3><p class="pub-authors"><span class="me">Ling Yang</span>+, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, Bin Cui</p><div class="pub-links"><a href="https://arxiv.org/abs/2407.02398">Paper</a><a href="https://github.com/YangLing0818/consistency_flow_matching">Code</a><a href="https://x.com/LingYang_PKU/status/1808509588414800224">Tweet</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag iclr">ICLR 2024</span></div><h3>Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing</h3><p class="pub-authors"><span class="me">Ling Yang</span>, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui</p><div class="pub-links"><a href="https://openreview.net/forum?id=nFMS6wF2xq">Paper</a><a href="https://github.com/YangLing0818/ContextDiff">Code</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag cvpr">CVPR 2024</span></div><h3>Structure-Guided Adversarial Training of Diffusion Models</h3><p class="pub-authors"><span class="me">Ling Yang</span>, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui</p><div class="pub-links"><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Structure-Guided_Adversarial_Training_of_Diffusion_Models_CVPR_2024_paper.pdf">Paper</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag neurips">NeurIPS 2023</span></div><h3>Improving Diffusion-Based Image Synthesis with Context Prediction</h3><p class="pub-authors"><span class="me">Ling Yang</span>, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, Bin Cui</p><div class="pub-links"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/7664a7e946a84ac5e97649a967717cf2-Abstract-Conference.html">Paper</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag acm">ACM Computing Surveys 2023</span></div><h3>Diffusion Models: A Comprehensive Survey of Methods and Applications</h3><p class="pub-authors"><span class="me">Ling Yang</span>, Zhilong Zhang, Yang Song (OpenAI), Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, Ming-Hsuan Yang</p><div class="pub-links"><a href="https://arxiv.org/abs/2209.00796">Paper</a><a href="https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy">Code</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag neurips">NeurIPS 2024</span></div><h3>VideoTetris: Towards Compositional Text-to-Video Generation</h3><p class="pub-authors">Ye Tian*, <span class="me">Ling Yang</span>*+, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, Bin Cui</p><div class="pub-links"><a href="https://arxiv.org/abs/2406.04277">Paper</a><a href="https://github.com/YangLing0818/VideoTetris">Code</a><a href="https://x.com/_akhaliq/status/1798897351534489608">Tweet</a></div></div>
  </div>

  <div class="pub-section-title">Additional Selected Publications</div>
  <div class="pub-list">
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag iclr">ICLR 2024</span></div><h3>VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs</h3><p class="pub-authors"><span class="me">Ling Yang</span>, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec</p><div class="pub-links"><a href="https://openreview.net/forum?id=h6Tz85BqRI">Paper</a><a href="https://github.com/YangLing0818/VQGraph">Code</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag cvpr">CVPR 2020</span></div><h3>DPGN: Distribution Propagation Graph Network for Few-Shot Learning</h3><p class="pub-authors"><span class="me">Ling Yang</span>, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, Yu Liu</p><div class="pub-links"><a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.html">Paper</a><a href="https://github.com/megvii-research/DPGN">Code</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag icml">ICML 2022</span><span class="pub-award-tag">Spotlight Top 3%</span></div><h3>Unsupervised Time-Series Representation Learning with Iterative Bilinear Temporal-Spectral Fusion</h3><p class="pub-authors"><span class="me">Ling Yang</span>+, Shenda Hong</p><div class="pub-links"><a href="https://proceedings.mlr.press/v162/yang22e.html">Paper</a></div></div>
    <div class="pub-item"><div class="pub-meta"><span class="pub-venue-tag emnlp">EMNLP 2025</span><span class="pub-award-tag">Oral Top 1%</span></div><h3>EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety</h3><p class="pub-authors">Jiahao Qiu, Yinghui He, Xinzhe Juan, Yimin Wang, Yuhan Liu, Zixin Yao, Yue Wu, Xun Jiang, <span class="me">Ling Yang</span>, Mengdi Wang</p><div class="pub-links"><a href="https://arxiv.org/abs/2504.09689">Paper</a><a href="https://github.com/1akaman/EmoAgent">Code</a></div></div>
  </div>
</section>

<section id="talks">
  <div class="section-header"><h2>Invited Talks</h2><div class="line"></div></div>
  <div class="talks-list">
    <div class="talk-item"><span class="talk-year">2025</span>Talk at ICCV 2025 Workshop on <a href="https://agent-intelligence.github.io/agent-intelligence/">MMRAgI</a></div>
    <div class="talk-item"><span class="talk-year">2025</span>Talk at <a href="http://english.ia.cas.cn/">Institute of Automation of the Chinese Academy of Sciences</a></div>
    <div class="talk-item"><span class="talk-year">2025</span>Roundtable forum at WAIC 2025, hosted by Prof. <a href="https://scholar.google.com/citations?user=GMzzRRUAAAAJ&hl=en">Dahua Lin</a></div>
    <div class="talk-item"><span class="talk-year">2025</span>Talk at <a href="https://www.shlab.org.cn/">Shanghai Artificial Intelligence Laboratory</a></div>
    <div class="talk-item"><span class="talk-year">2024</span>Talk at <a href="https://ai.princeton.edu/ai-lab">Princeton AI Lab</a>, hosted by Prof. <a href="https://ece.princeton.edu/people/mengdi-wang">Mengdi Wang</a></div>
  </div>
</section>

<section id="honors">
  <div class="section-header"><h2>Honors &amp; Awards</h2><div class="line"></div></div>
  <div class="honors-grid">
    <div class="honor-item"><div class="honor-icon">üèÜ</div><div class="honor-text"><h4>2025 WAIC Yunfan Award Finalist</h4><p>10 selected worldwide &middot; 2025</p></div></div>
    <div class="honor-item"><div class="honor-icon">üéì</div><div class="honor-text"><h4>Outstanding Graduate</h4><p>Peking University Ph.D. &middot; 2025</p></div></div>
    <div class="honor-item"><div class="honor-icon">‚≠ê</div><div class="honor-text"><h4>KAUST Rising Stars in AI Symposium</h4><p>24 selected worldwide &middot; 2025</p></div></div>
    <div class="honor-item"><div class="honor-icon">üé§</div><div class="honor-text"><h4>VALSE Distinguished Student Forum</h4><p>8 selected in China &middot; 2024</p></div></div>
    <div class="honor-item"><div class="honor-icon">üìñ</div><div class="honor-text"><h4>Outstanding Author</h4><p>Electronics Industry Press &middot; 2023</p></div></div>
    <div class="honor-item"><div class="honor-icon">üèÖ</div><div class="honor-text"><h4>National Scholarship for Ph.D. Students</h4><p>Top 1% at PKU &middot; 2022</p></div></div>
    <div class="honor-item"><div class="honor-icon">üí°</div><div class="honor-text"><h4>Exceptional Award for Academic Innovation</h4><p>Top 1% at PKU &middot; 2022</p></div></div>
  </div>
</section>

<section id="services">
  <div class="section-header"><h2>Academic Services</h2><div class="line"></div></div>
  <div class="services-grid">
    <div class="service-group"><h3>Area Chair</h3><p>NeurIPS, ICLR</p></div>
    <div class="service-group"><h3>Program Committee / Reviewer</h3><ul><li>ICML, ICLR, CVPR, ICCV, AAAI 2025</li><li>SIGGRAPH, ICML, ICLR, NeurIPS, CVPR, AAAI 2024</li><li>ICML, ICLR, NeurIPS, CVPR, AAAI 2023</li><li>ICML, ICLR, NeurIPS 2022</li></ul></div>
    <div class="service-group"><h3>Journal Reviewer</h3><ul><li>ACM Computing Surveys (CSUR)</li><li>IEEE TPAMI</li><li>IEEE TKDE</li><li>IEEE TCSVT</li><li>IEEE TNNLS</li><li>Pattern Recognition (PR)</li></ul></div>
  </div>
</section>

<footer>
  <p>&copy; 2026 Ling Yang &middot; Princeton University</p>
  <div class="foot-links">
    <a href="/cdn-cgi/l/email-protection#7d041c131a1114131a4d454c453d4c4b4e531e1210">Email</a>
    <a href="https://scholar.google.com.hk/citations?user=sIKujqAAAAAJ&hl=en">Google Scholar</a>
    <a href="https://github.com/Gen-Verse">GitHub</a>
    <a href="https://x.com/LingYang_PU">Twitter/X</a>
    <a href="https://YangLing0818.github.io/feed.xml">RSS Feed</a>
  </div>
</footer>

<script>
function toggleNews(){
  var items = document.querySelectorAll('.hidden-news');
  var btn = document.querySelector('.show-more-btn');
  var isHidden = items[0].style.display === '' || items[0].style.display === 'none' || !items[0].classList.contains('show');
  items.forEach(function(el){ el.classList.toggle('show'); });
  btn.innerHTML = isHidden ? 'Show less &uarr;' : 'Show more &darr;';
}
document.getElementById('navbar').querySelector('.mobile-toggle') && document.addEventListener('click', function(e){
  var nav = document.getElementById('navbar');
  if(!nav.contains(e.target)) nav.classList.remove('open');
});
</script>
</body>
</html>
