<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Building Diffusion Language Models — Ling Yang, Princeton University</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Instrument+Serif:ital@0;1&family=DM+Sans:ital,opsz,wght@0,9..40,300..700;1,9..40,300..700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
<style>
:root {
  --bg: #faf9f7; --bg-card: #ffffff; --text: #1a1a1a; --text-secondary: #555;
  --text-muted: #888; --accent: #2563eb; --accent-warm: #c2410c;
  --border: #e8e5e0; --border-light: #f0ede8; --tag-bg: #f0ede8;
  --shadow-sm: 0 1px 3px rgba(0,0,0,0.04); --shadow-md: 0 4px 20px rgba(0,0,0,0.06);
  --radius: 12px; --radius-sm: 8px;
  --serif: 'Instrument Serif', Georgia, serif;
  --sans: 'DM Sans', -apple-system, sans-serif;
  --mono: 'JetBrains Mono', monospace;
  --max-w: 1080px;
  --tag-mmada: #0891b2; --tag-mmada-bg: #cffafe;
  --tag-para: #7c3aed; --tag-para-bg: #ede9fe;
  --tag-trado: #dc2626; --tag-trado-bg: #fee2e2;
}
* { margin:0; padding:0; box-sizing:border-box; }
html { scroll-behavior:smooth; }
body { font-family:var(--sans); background:var(--bg); color:var(--text); line-height:1.7; font-size:17px; -webkit-font-smoothing:antialiased; }
::selection { background:#2563eb22; }
a { color:var(--accent); text-decoration:none; transition:color .2s; }
a:hover { color:var(--accent-warm); }

nav { position:sticky; top:0; z-index:100; background:rgba(250,249,247,.85); backdrop-filter:blur(16px); -webkit-backdrop-filter:blur(16px); border-bottom:1px solid var(--border-light); }
nav .inner { max-width:var(--max-w); margin:0 auto; display:flex; align-items:center; justify-content:space-between; padding:14px 32px; }
nav .logo { font-family:var(--serif); font-size:20px; color:var(--text); letter-spacing:-.02em; }
nav .links { display:flex; gap:24px; flex-wrap:wrap; }
nav .links a { font-size:14.5px; font-weight:500; color:var(--text-secondary); letter-spacing:.01em; position:relative; transition:color .2s; }
nav .links a:hover { color:var(--text); }
nav .links a.active { color:var(--text); }
nav .links a.active::after { content:''; position:absolute; bottom:-4px; left:0; right:0; height:1.5px; background:var(--text); border-radius:1px; }
nav .mobile-toggle { display:none; background:none; border:none; font-size:22px; cursor:pointer; }
nav .princeton-badge { display:flex; align-items:center; gap:8px; text-decoration:none; border-bottom:none !important; }
nav .princeton-badge img { height:90px; width:auto; opacity:.9; }
nav .princeton-badge:hover img { opacity:1; }

.blog-hero { max-width:780px; margin:0 auto; padding:64px 32px 0; }
.blog-back { font-size:14px; color:var(--text-muted); display:inline-flex; align-items:center; gap:4px; margin-bottom:28px; transition:color .2s; }
.blog-back:hover { color:var(--accent); }
.blog-date { font-family:var(--mono); font-size:13px; color:var(--text-muted); letter-spacing:.5px; text-transform:uppercase; margin-bottom:16px; }
.blog-hero h1 { font-family:var(--serif); font-size:clamp(2rem, 4.5vw, 2.8rem); font-weight:400; line-height:1.18; letter-spacing:-.02em; margin-bottom:18px; }
.blog-hero h1 em { font-style:italic; color:var(--accent); }
.blog-subtitle { font-size:17.5px; color:var(--text-secondary); line-height:1.7; margin-bottom:20px; }
.blog-authors { font-size:15px; color:var(--text-secondary); margin-bottom:6px; }
.blog-authors a { border-bottom:1px solid #2563eb33; }
.blog-authors a:hover { border-bottom-color:var(--accent); }
.blog-affil { font-size:13.5px; color:var(--text-muted); margin-bottom:24px; }
.blog-tags { display:flex; gap:8px; flex-wrap:wrap; margin-bottom:12px; }
.tag { font-family:var(--mono); font-size:12.5px; padding:5px 14px; border-radius:100px; font-weight:500; display:inline-flex; align-items:center; gap:5px; transition:transform .15s; }
.tag:hover { transform:translateY(-1px); }
.tag-mmada { background:var(--tag-mmada-bg); color:var(--tag-mmada); }
.tag-para { background:var(--tag-para-bg); color:var(--tag-para); }
.tag-trado { background:var(--tag-trado-bg); color:var(--tag-trado); }
.blog-divider { width:48px; height:1.5px; background:var(--border); margin:40px auto; }

article { max-width:720px; margin:0 auto; padding:0 32px 80px; }
article h2 { font-family:var(--serif); font-size:1.75rem; font-weight:400; margin:52px 0 16px; line-height:1.25; letter-spacing:-.01em; }
article h3 { font-size:1.05rem; font-weight:600; margin:32px 0 10px; letter-spacing:-.01em; }
article p { margin-bottom:16px; text-align:justify; }
article a { border-bottom:1px solid transparent; transition:border-color .2s; }
article a:hover { border-color:var(--accent); }
article strong { font-weight:600; }
article ul, article ol { margin:0 0 16px 20px; }
article li { margin-bottom:6px; }

.callout { background:var(--bg-card); border:1px solid var(--border); border-radius:var(--radius); padding:24px 28px; margin:28px 0; position:relative; overflow:hidden; transition:box-shadow .3s; }
.callout:hover { box-shadow:var(--shadow-md); }
.callout::before { content:''; position:absolute; left:0; top:0; bottom:0; width:4px; }
.callout-mmada::before { background:var(--tag-mmada); }
.callout-para::before { background:var(--tag-para); }
.callout-trado::before { background:var(--tag-trado); }
.callout-label { font-family:var(--mono); font-size:12px; font-weight:600; text-transform:uppercase; letter-spacing:1px; margin-bottom:6px; }
.callout-mmada .callout-label { color:var(--tag-mmada); }
.callout-para .callout-label { color:var(--tag-para); }
.callout-trado .callout-label { color:var(--tag-trado); }
.callout-title { font-family:var(--serif); font-size:1.25rem; margin-bottom:8px; line-height:1.3; }
.callout p { font-size:14.5px; color:var(--text-secondary); margin-bottom:10px; line-height:1.6; text-align:left; }
.callout-links { display:flex; gap:5px; flex-wrap:wrap; }
.callout-links a { font-family:var(--mono); font-size:12.5px; padding:3px 10px; border-radius:5px; background:var(--tag-bg); color:var(--text-secondary); border:1px solid transparent; transition:all .2s; }
.callout-links a:hover { background:var(--bg); border-color:var(--border); color:var(--accent); }

.stats-row { display:grid; grid-template-columns:repeat(auto-fit, minmax(170px,1fr)); gap:12px; margin:24px 0; }
.stat-card { background:var(--bg-card); border:1px solid var(--border); border-radius:var(--radius-sm); padding:18px; text-align:center; transition:all .2s; }
.stat-card:hover { box-shadow:var(--shadow-sm); }
.stat-num { font-family:var(--serif); font-size:1.85rem; font-weight:400; line-height:1.1; }
.stat-num.cyan { color:var(--tag-mmada); }
.stat-num.purple { color:var(--tag-para); }
.stat-num.red { color:var(--tag-trado); }
.stat-label { font-size:13px; color:var(--text-muted); margin-top:4px; }

.timeline { position:relative; margin:36px 0; padding-left:30px; }
.timeline::before { content:''; position:absolute; left:7px; top:8px; bottom:8px; width:2px; background:linear-gradient(to bottom, var(--tag-mmada), var(--tag-para), var(--tag-trado)); border-radius:2px; }
.tl-item { position:relative; margin-bottom:24px; }
.tl-item::before { content:''; position:absolute; left:-26px; top:8px; width:10px; height:10px; border-radius:50%; border:2px solid; background:var(--bg); }
.tl-mmada::before { border-color:var(--tag-mmada); }
.tl-para::before { border-color:var(--tag-para); }
.tl-trado::before { border-color:var(--tag-trado); }
.tl-time { font-family:var(--mono); font-size:12px; color:var(--text-muted); margin-bottom:2px; }
.tl-title { font-weight:600; font-size:15.5px; margin-bottom:3px; }
.tl-desc { font-size:14.5px; color:var(--text-secondary); line-height:1.6; }

blockquote { border-left:3px solid var(--accent); padding:4px 0 4px 20px; margin:24px 0; color:var(--text-secondary); font-style:italic; }
code { font-family:var(--mono); font-size:.88em; background:var(--border-light); padding:2px 6px; border-radius:4px; }
pre { background:var(--border-light); padding:16px 20px; border-radius:var(--radius-sm); font-size:13px; font-family:var(--mono); overflow-x:auto; line-height:1.6; color:#444; margin:20px 0; }

.detail-box { background:var(--bg-card); border:1px solid var(--border); border-radius:var(--radius); padding:22px 26px; margin:24px 0; }
.detail-box h4 { font-size:15px; font-weight:600; margin-bottom:8px; display:flex; align-items:center; gap:8px; }
.detail-box h4::before { content:''; width:3px; height:14px; border-radius:2px; }
.detail-box.mmada h4::before { background:var(--tag-mmada); }
.detail-box.para h4::before { background:var(--tag-para); }
.detail-box.trado h4::before { background:var(--tag-trado); }
.detail-box p, .detail-box ul { font-size:15px; color:var(--text-secondary); line-height:1.7; }
.detail-box ul { margin-left:18px; }

footer { max-width:var(--max-w); margin:0 auto; padding:28px 32px; border-top:1px solid var(--border); display:flex; justify-content:space-between; align-items:center; flex-wrap:wrap; gap:12px; }
footer p { font-size:13.5px; color:var(--text-muted); }
footer .foot-links { display:flex; gap:16px; flex-wrap:wrap; }
footer .foot-links a { font-size:13.5px; color:var(--text-muted); }
footer .foot-links a:hover { color:var(--text); }

@media (max-width:768px) {
  .blog-hero { padding:40px 20px 0; }
  .blog-hero h1 { font-size:1.7rem; }
  article { padding:0 20px 60px; }
  article h2 { font-size:1.4rem; }
  .stats-row { grid-template-columns:1fr 1fr; }
  nav .links { display:none; }
  nav .mobile-toggle { display:block; }
  nav.open .links { display:flex; flex-direction:column; position:absolute; top:100%; left:0; right:0; background:var(--bg); padding:16px 32px; border-bottom:1px solid var(--border); gap:12px; }
}
</style>
</head>
<body>

<nav id="navbar">
  <div class="inner">
    <a href="index.html" class="logo">Ling Yang</a>
    <button class="mobile-toggle" onclick="document.getElementById('navbar').classList.toggle('open')">&#9776;</button>
    <div class="links">
      <a href="index.html">About</a>
      <a href="index.html#research">Research</a>
      <a href="index.html#news">News</a>
      <a href="index.html#publications">Publications</a>
      <a href="#" class="active">Blog</a>
    </div>
    <a href="https://www.princeton.edu" class="princeton-badge" target="_blank">
      <img src="https://yangling0818.github.io/images/prince.png" alt="Princeton University">
    </a>
  </div>
</nav>

<div class="blog-hero">
  <a href="index.html" class="blog-back">&larr; Back to Home</a>
  <div class="blog-date">2025</div>
  <h1>Building <em>Diffusion Language Models</em> That Reason, See, and Generate</h1>
  <p class="blog-subtitle">Three tightly connected projects — TraceRL/TraDo, MMaDA, and MMaDA-Parallel — that build a complete diffusion language model stack from scratch: first an RL framework for reasoning, then a unified multimodal architecture, and finally parallel thinking-aware generation.</p>
  <p class="blog-authors"><a href="https://yangling0818.github.io">Ling Yang</a></p>
  <p class="blog-affil">Gen-Verse &middot; Princeton AI Lab &middot; Princeton University</p>
  <div class="blog-tags">
    <span class="tag tag-trado">TraceRL / TraDo</span>
    <span class="tag tag-mmada">MMaDA</span>
    <span class="tag tag-para">MMaDA-Parallel</span>
  </div>
</div>

<div class="blog-divider"></div>

<article>

<p>
Diffusion language models (dLLMs) represent a fundamentally different way to generate text: instead of predicting tokens left to right, they denoise an entire sequence in parallel. This architectural choice opens the door to genuinely new capabilities — bidirectional reasoning, parallel multimodal generation, and flexible inference-time compute allocation — but it also demands a new training and post-training stack built from the ground up.
</p>
<p>
Over the past year, our team has built that stack, one layer at a time. The three projects in this blog form a single, continuous engineering effort. <strong>TraceRL</strong> solves the foundational problem of how to do RL for dLLMs at all — aligning post-training with the model's actual inference trajectory. <strong>MMaDA</strong> then uses this RL infrastructure to build the first unified multimodal diffusion model that can reason over text, understand images, and generate images under a single architecture. <strong>MMaDA-Parallel</strong> takes the final step: it enables text and images to be denoised simultaneously, with bidirectional attention at every step, solving the error propagation problem that plagues sequential thinking-aware generation.
</p>
<p>
Each project directly depends on the one before it. TraceRL's trajectory-aware training became MMaDA's UniGRPO algorithm; MMaDA's unified discrete diffusion architecture became MMaDA-Parallel's backbone; and MMaDA-Parallel's ParaRL strategy extends the RL toolkit to dense trajectory-level rewards. This blog walks through the three projects in that order.
</p>

<!-- Timeline -->
<div class="timeline">
  <div class="tl-item tl-trado">
    <div class="tl-time">Sep 2025 · Preprint</div>
    <div class="tl-title">Step 1: TraceRL / TraDo — RL Infrastructure for dLLMs</div>
    <div class="tl-desc">A trajectory-aware RL framework that aligns post-training with diffusion inference traces, plus a diffusion-based value model for stability. Produces SOTA dLLMs (TraDo-4B/8B) that beat 7B AR models.</div>
  </div>
  <div class="tl-item tl-mmada">
    <div class="tl-time">May 2025 · NeurIPS 2025</div>
    <div class="tl-title">Step 2: MMaDA — Unified Multimodal Diffusion Foundation</div>
    <div class="tl-desc">Building on TraceRL's RL toolkit, MMaDA unifies text reasoning, multimodal understanding, and image generation under a single diffusion architecture with shared probabilistic formulation and UniGRPO post-training.</div>
  </div>
  <div class="tl-item tl-para">
    <div class="tl-time">Nov 2025 · Preprint</div>
    <div class="tl-title">Step 3: MMaDA-Parallel — Parallel Thinking-Aware Generation</div>
    <div class="tl-desc">Inheriting MMaDA's architecture, enables bidirectional text-image denoising at every step. ParaRL applies semantic rewards along the trajectory to enforce cross-modal consistency.</div>
  </div>
</div>


<!-- ========== TraceRL / TraDo ========== -->
<h2>TraceRL &amp; TraDo: Reinventing RL for Diffusion Language Models</h2>

<div class="callout callout-trado">
  <div class="callout-label">TraceRL / TraDo</div>
  <div class="callout-title">Trajectory-Aware RL Framework for dLLMs</div>
  <p>A comprehensive framework for building, training, and deploying diffusion LLMs across full-attention and block-attention architectures, with SOTA models TraDo-4B/8B.</p>
  <div class="callout-links">
    <a href="https://arxiv.org/abs/2509.06949" target="_blank">Paper</a>
    <a href="https://github.com/Gen-Verse/dLLM-RL" target="_blank">GitHub</a>
  </div>
</div>

<h3>The Core Problem: Train-Inference Mismatch</h3>
<p>
Traditional dLLM training corrupts tokens with random masking, but inference follows a structured, sequential unmasking trajectory — high-confidence tokens are revealed first, then progressively harder ones. This creates a fundamental mismatch: the model is trained on uniformly random corruption patterns but must perform well on the structured patterns it encounters during inference.
</p>

<div class="detail-box trado">
<h4>TraceRL: Trajectory-Aware Training</h4>
<p>
TraceRL closes this gap by incorporating the model's preferred inference trajectory directly into RL post-training. During each training iteration, the framework first samples a complete inference trace — the sequence of denoising steps the model would actually execute — then uses this trace to construct the training objective. This means the model is optimized on the exact corruption-to-clean patterns it will encounter at test time, rather than on random masks.
</p>
<p>
A key innovation is the <strong>shrinkage parameter</strong> that controls how many trajectory steps are used in each training update. Rather than backpropagating through every denoising step (which is prohibitively expensive), TraceRL selects a sparse subset of steps along the trajectory, reducing training complexity by a factor of <code>s</code> while preserving gradient quality.
</p>
</div>

<div class="detail-box trado">
<h4>Diffusion-Based Value Model</h4>
<p>
Vanilla policy-gradient methods suffer from high variance in the diffusion setting because a single trajectory contains dozens of denoising steps with correlated noise. TraceRL introduces a diffusion-based value model that estimates the expected return at each step of the denoising trajectory, providing per-step baselines that dramatically reduce gradient variance. This value model also naturally accommodates process reward models (PRMs), enabling fine-grained supervision at intermediate denoising steps.
</p>
</div>

<div class="detail-box trado">
<h4>Engineering: The dLLM-RL Framework</h4>
<p>
Beyond the algorithm, the release includes a full-stack open-source framework supporting: (1) multiple RL methods — TraceRL, coupled RL, and random-masking RL; (2) both full-attention (LLaDA-style) and block-attention (SDAR-style) dLLM architectures; (3) accelerated inference via improved KV-cache and JetEngine integration; (4) SFT pipelines including block SFT, semi-AR SFT, and long-CoT fine-tuning with multi-node support; (5) block-size adaptation — TraceRL can adapt a model trained on block size <code>B=4</code> to <code>B=8</code>, improving sampling flexibility without retraining from scratch.
</p>
</div>

<div class="stats-row">
  <div class="stat-card"><div class="stat-num red">+6.1%</div><div class="stat-label">vs Qwen2.5-7B on math</div></div>
  <div class="stat-card"><div class="stat-num red">+51.3%</div><div class="stat-label">vs Llama3.1-8B on math</div></div>
  <div class="stat-card"><div class="stat-num red">+18.1%</div><div class="stat-label">1st long-CoT dLLM on MATH500</div></div>
</div>

<p>
TraDo-4B-Instruct, despite being smaller than 7B-scale AR models, consistently outperforms them on complex math reasoning. TraDo-8B-Thinking is the first long-CoT diffusion language model, trained via curriculum learning that progressively extends reasoning length. These results establish that diffusion language models are not just a theoretical curiosity — with the right RL framework, they can match and beat the best autoregressive models at reasoning.
</p>


<!-- ========== MMaDA ========== -->
<h2>MMaDA: One Architecture, Three Modalities</h2>

<div class="callout callout-mmada">
  <div class="callout-label">MMaDA</div>
  <div class="callout-title">Multimodal Large Diffusion Language Models</div>
  <p>The first unified diffusion foundation model that achieves strong performance on textual reasoning, multimodal understanding, and text-to-image generation — all under a single architecture and training objective.</p>
  <div class="callout-links">
    <a href="https://arxiv.org/abs/2505.15809" target="_blank">Paper</a>
    <a href="https://github.com/Gen-Verse/MMaDA" target="_blank">GitHub</a>
    <a href="https://huggingface.co/Gen-Verse/MMaDA-8B-Base" target="_blank">Models</a>
  </div>
</div>

<h3>Unified Discrete Diffusion Architecture</h3>
<p>
Most multimodal models bolt together modality-specific components — separate encoders, decoders, and loss functions for text and images. MMaDA takes a radically different approach: both text and images are represented as discrete tokens, and a single masked diffusion process operates over the combined token space. The model's training objective is simply to predict masked tokens, regardless of whether they represent words or image patches. This eliminates modality-specific engineering and enables genuine cross-modal reasoning during denoising.
</p>

<div class="detail-box mmada">
<h4>Shared Probabilistic Formulation</h4>
<p>
Text is tokenized using the LLaDA tokenizer; images are encoded into discrete visual tokens via a VQ codebook. Both are arranged into a single sequence and corrupted by the same masked diffusion forward process. The reverse process — a single Transformer predicting all masked tokens — recovers both text and image content simultaneously. This shared formulation ensures that the model learns cross-modal interactions naturally during pretraining, without explicit alignment losses or contrastive objectives.
</p>
</div>

<h3>Mixed Long-CoT Fine-Tuning</h3>
<p>
To enable complex reasoning after pretraining, MMaDA introduces a mixed long chain-of-thought (CoT) fine-tuning strategy. The key insight is that CoT reasoning should be unified across modalities: the model learns to produce step-by-step reasoning traces for math problems, image understanding questions, and even world-knowledge-aware image generation prompts, all in the same format. Diverse reasoning trajectories are generated using open-source LLMs and VLMs, then filtered by SOTA verifiers to retain only high-quality, long-form CoT samples.
</p>
<p>
This mixed-CoT stage serves a dual purpose: it teaches the model to reason, and it provides a strong initialization for the subsequent RL stage — a "cold start" that gives the RL algorithm a reasonable policy to improve upon rather than optimizing from random exploration.
</p>

<div class="detail-box mmada">
<h4>UniGRPO: Unified RL for Diffusion Models</h4>
<p>
Building directly on TraceRL's insights, MMaDA introduces UniGRPO — a unified policy-gradient-based RL algorithm tailored for multimodal diffusion models. UniGRPO extends GRPO to the diffusion setting with <strong>diversified reward modeling</strong>: different reward functions for different task types (math verifiers for reasoning, CLIP-based rewards for image generation, VLM-based rewards for multimodal understanding), all optimized under the same policy gradient framework. The random-masking RL variant from the dLLM-RL toolkit is used here, with rewards computed over the generated outputs and gradients propagated through the masked diffusion objective.
</p>
</div>

<div class="stats-row">
  <div class="stat-card"><div class="stat-num cyan">8B</div><div class="stat-label">parameters, one architecture</div></div>
  <div class="stat-card"><div class="stat-num cyan">&gt; LLaMA-3</div><div class="stat-label">on text reasoning</div></div>
  <div class="stat-card"><div class="stat-num cyan">&gt; SDXL</div><div class="stat-label">on image generation</div></div>
</div>

<p>
MMaDA-8B surpasses LLaMA-3-7B and Qwen2-7B on textual reasoning, outperforms Show-o and SEED-X on multimodal understanding, and exceeds SDXL and Janus on text-to-image generation. The three-stage pipeline — pretraining, mixed-CoT SFT, UniGRPO — is fully open-sourced, with checkpoints released at each stage: MMaDA-8B-Base, MMaDA-8B-MixCoT, and MMaDA-8B-Max (after RL).
</p>


<!-- ========== MMaDA-Parallel ========== -->
<h2>MMaDA-Parallel: Thinking and Generating in Lockstep</h2>

<div class="callout callout-para">
  <div class="callout-label">MMaDA-Parallel</div>
  <div class="callout-title">Parallel Thinking-Aware Image Editing and Generation</div>
  <p>A parallel multimodal diffusion framework that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory, solving the error propagation problem of sequential thinking.</p>
  <div class="callout-links">
    <a href="https://arxiv.org/abs/2511.09611" target="_blank">Paper</a>
    <a href="https://github.com/tyfeld/MMaDA-Parallel" target="_blank">GitHub</a>
    <a href="https://huggingface.co/tyfeld/MMaDA-Parallel-A" target="_blank">Models</a>
  </div>
</div>

<h3>The Problem: Sequential Thinking Can Hurt</h3>
<p>
"Thinking before generating" sounds intuitive — let the model reason in text, then produce an image conditioned on that reasoning. But MMaDA-Parallel identifies a critical failure mode: in sequential, autoregressive pipelines, errors in the reasoning text propagate irreversibly into the generated image. If the CoT produces a wrong spatial description, the image generator has no way to recover. The authors introduce <strong>ParaBench</strong>, a new benchmark evaluating both text and image outputs, and show that this performance degradation correlates strongly with poor alignment between generated reasoning and the final image.
</p>

<div class="detail-box para">
<h4>Parallel Denoising Architecture</h4>
<p>
The solution is architectural: instead of generating text first, then images, MMaDA-Parallel denoise both modalities simultaneously. Text and image tokens are arranged in an interleaved sequence with bidirectional attention, and a single mask predictor operates over the full combined sequence at each denoising step. This means at step <code>t</code>, the partially denoised text can attend to the partially denoised image and vice versa — creating a continuous feedback loop where semantic concepts in text and their visual counterparts co-evolve together.
</p>
<p>
The authors observe a striking emergent behavior: during parallel denoising, the image region corresponding to a specific semantic concept is often refined simultaneously with its textual counterpart. The model naturally learns to synchronize cross-modal information without explicit supervision of this alignment.
</p>
</div>

<div class="detail-box para">
<h4>ParaRL: Reinforcement Learning Along the Trajectory</h4>
<p>
Standard SFT and conventional RL algorithms optimize only for the final output quality. ParaRL goes further: it applies semantic rewards at multiple points along the denoising trajectory, enforcing cross-modal consistency not just in the final result but throughout the generation process.
</p>
<p>
Since computing rewards at every single step is prohibitively expensive, ParaRL adopts a <strong>sparse optimization strategy</strong>: during each online rollout, a fixed subset of step indices <code>S ⊂ {1, ..., |τ|}</code> is pre-selected, and rewards are computed only at those steps. The method adapts a diffusion GRPO objective with token-level likelihood ratios, standardizing advantages across the sparsely sampled steps to maintain gradient quality.
</p>
</div>

<div class="detail-box para">
<h4>Data Curation for Parallel Thinking</h4>
<p>
Training a parallel thinking-aware model requires quadruplets — (input image, instruction, reasoning trace, output image) — that don't exist in standard datasets. The team curates this data by taking existing image editing datasets and using Qwen-2.5-VL to generate plausible reasoning traces connecting instructions to output images. This synthetic data is used for supervised fine-tuning on the MMaDA backbone before applying ParaRL.
</p>
<p>
Two model variants are released: <strong>MMaDA-Parallel-A</strong> (based on Amused-VQ tokenizer, trained from Lumina-DiMOO) and <strong>MMaDA-Parallel-M</strong> (based on MagVITv2, trained from MMaDA), offering different quality-robustness trade-offs.
</p>
</div>

<div class="stats-row">
  <div class="stat-card"><div class="stat-num purple">+6.9%</div><div class="stat-label">Output Alignment vs Bagel</div></div>
  <div class="stat-card"><div class="stat-num purple">SOTA</div><div class="stat-label">on ParaBench (open-source)</div></div>
  <div class="stat-card"><div class="stat-num purple">2</div><div class="stat-label">released 8B model variants</div></div>
</div>


<!-- ========== Unified Philosophy ========== -->
<h2>The Common Thread</h2>

<blockquote>Diffusion language models are not autoregressive models with a different sampling strategy — they are a genuinely new computational paradigm that demands new training algorithms, new multimodal architectures, and new approaches to thinking-aware generation.</blockquote>

<p>
Across these three projects, a consistent philosophy emerges: take what works for autoregressive LLMs and reimagine it natively for the diffusion setting. TraceRL reimagines RL training by respecting the denoising trajectory. MMaDA reimagines multimodal modeling by using a single diffusion process over discrete tokens. MMaDA-Parallel reimagines thinking-aware generation by replacing sequential reasoning with parallel co-denoising.
</p>
<p>
The result is a complete, open-source stack — from low-level RL algorithms to high-level multimodal applications — that demonstrates dLLMs can match or exceed autoregressive models across text reasoning, multimodal understanding, image generation, and thinking-aware editing. And because diffusion models decode in parallel, they open capabilities (like bidirectional cross-modal attention at every step) that are simply impossible in the autoregressive paradigm.
</p>


<!-- ========== Citation ========== -->
<div style="margin-top:48px; padding-top:28px; border-top:1px solid var(--border);">
<h3 style="margin-top:0;">Citation</h3>
<p style="font-size:14px; color:var(--text-muted); margin-bottom:12px;">If you find our work useful, please consider citing:</p>
<pre style="margin-left:0; margin-right:0; width:100%; box-sizing:border-box; white-space:pre-wrap; word-break:break-all;">@article{wang2025tracerl,
  title={Revolutionizing Reinforcement Learning Framework
         for Diffusion Large Language Models},
  author={Wang, Yinjie and Yang, Ling and Li, Bowen
          and Tian, Ye and Shen, Ke and Wang, Mengdi},
  journal={arXiv preprint arXiv:2509.06949},
  year={2025}
}

@article{yang2025mmada,
  title={MMaDA: Multimodal Large Diffusion
         Language Models},
  author={Yang, Ling and Tian, Ye and Li, Bowen
          and Zhang, Xinchen and Shen, Ke
          and Tong, Yunhai and Wang, Mengdi},
  journal={arXiv preprint arXiv:2505.15809},
  year={2025}
}

@article{tian2025mmadaparallel,
  title={MMaDA-Parallel: Multimodal Large Diffusion
         Language Models for Thinking-Aware Editing
         and Generation},
  author={Tian, Ye and Yang, Ling and Yang, Jiongfan
          and Wang, Anran and Tian, Yu and Zheng, Jiani
          and Wang, Haochen and Teng, Zhiyang
          and Wang, Zhuochen and Wang, Yinjie
          and Tong, Yunhai and Wang, Mengdi
          and Li, Xiangtai},
  journal={arXiv preprint arXiv:2511.09611},
  year={2025}
}</pre>
</div>

</article>

<footer>
  <p>&copy; 2025 Ling Yang &middot; Princeton University</p>
  <div class="foot-links">
    <a href="mailto:yangling0818@163.com">Email</a>
    <a href="https://scholar.google.com.hk/citations?user=sIKujqAAAAAJ&hl=en">Google Scholar</a>
    <a href="https://github.com/Gen-Verse">GitHub</a>
    <a href="https://x.com/LingYang_PU">Twitter/X</a>
  </div>
</footer>

<script>
document.addEventListener('click', function(e){
  var nav = document.getElementById('navbar');
  if(!nav.contains(e.target)) nav.classList.remove('open');
});
</script>
</body>
</html>
